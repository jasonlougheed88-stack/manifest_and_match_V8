#!/usr/bin/env swift

// Performance Monitoring System Validation - ManifestAndMatchV7
// Validates real-time monitoring and alerting capabilities

import Foundation

print("ðŸ“Š ManifestAndMatchV7 Performance Monitoring Validation")
print("====================================================")
print("")

// Monitoring System Configuration
struct MonitoringConfig {
    let thompsonThresholdMs: Double = 10.0
    let memoryBaselineMB: Double = 200.0
    let memoryWarningMB: Double = 170.0
    let memoryCriticalMB: Double = 190.0
    let apiTimeoutMs: Double = 5000.0
    let alertingEnabled: Bool = true
    let metricsRetentionHours: Int = 24
}

// Performance Metrics Structure
struct PerformanceMetrics {
    let timestamp: Date
    let thompsonResponseTime: Double    // ms
    let memoryUsage: Double            // MB
    let apiResponseTime: Double        // ms
    let cacheHitRate: Double          // 0-1
    let errorRate: Double             // 0-1
    let throughput: Double            // jobs/second
}

// Alert System
enum AlertLevel {
    case info
    case warning
    case critical
    case emergency
}

struct Alert {
    let level: AlertLevel
    let metric: String
    let value: Double
    let threshold: Double
    let timestamp: Date
    let message: String
}

class PerformanceMonitoringSystem {
    private let config = MonitoringConfig()
    private var metricsHistory: [PerformanceMetrics] = []
    private var alerts: [Alert] = []

    func recordMetrics(_ metrics: PerformanceMetrics) {
        metricsHistory.append(metrics)

        // Trim history to retention period
        let cutoff = Date().addingTimeInterval(-Double(config.metricsRetentionHours) * 3600)
        metricsHistory = metricsHistory.filter { $0.timestamp > cutoff }

        // Check for alert conditions
        checkAlertConditions(metrics)
    }

    private func checkAlertConditions(_ metrics: PerformanceMetrics) {
        // Thompson Performance Alerts
        if metrics.thompsonResponseTime > config.thompsonThresholdMs {
            let alert = Alert(
                level: .emergency,
                metric: "Thompson Response Time",
                value: metrics.thompsonResponseTime,
                threshold: config.thompsonThresholdMs,
                timestamp: metrics.timestamp,
                message: "ðŸš¨ CRITICAL: Sacred Thompson threshold violated!"
            )
            alerts.append(alert)
        } else if metrics.thompsonResponseTime > config.thompsonThresholdMs * 0.8 {
            let alert = Alert(
                level: .warning,
                metric: "Thompson Response Time",
                value: metrics.thompsonResponseTime,
                threshold: config.thompsonThresholdMs * 0.8,
                timestamp: metrics.timestamp,
                message: "âš ï¸ WARNING: Thompson performance approaching limit"
            )
            alerts.append(alert)
        }

        // Memory Usage Alerts
        if metrics.memoryUsage > config.memoryCriticalMB {
            let alert = Alert(
                level: .critical,
                metric: "Memory Usage",
                value: metrics.memoryUsage,
                threshold: config.memoryCriticalMB,
                timestamp: metrics.timestamp,
                message: "ðŸ”´ CRITICAL: Memory usage exceeds safe limits"
            )
            alerts.append(alert)
        } else if metrics.memoryUsage > config.memoryWarningMB {
            let alert = Alert(
                level: .warning,
                metric: "Memory Usage",
                value: metrics.memoryUsage,
                threshold: config.memoryWarningMB,
                timestamp: metrics.timestamp,
                message: "ðŸŸ¡ WARNING: Memory usage elevated"
            )
            alerts.append(alert)
        }

        // API Performance Alerts
        if metrics.apiResponseTime > config.apiTimeoutMs {
            let alert = Alert(
                level: .critical,
                metric: "API Response Time",
                value: metrics.apiResponseTime,
                threshold: config.apiTimeoutMs,
                timestamp: metrics.timestamp,
                message: "ðŸŒ CRITICAL: API response time exceeds timeout"
            )
            alerts.append(alert)
        }

        // Error Rate Alerts
        if metrics.errorRate > 0.05 { // 5% threshold
            let alert = Alert(
                level: .warning,
                metric: "Error Rate",
                value: metrics.errorRate * 100,
                threshold: 5.0,
                timestamp: metrics.timestamp,
                message: "âŒ WARNING: Error rate elevated"
            )
            alerts.append(alert)
        }
    }

    func getHealthStatus() -> (overall: String, score: Int, issues: [String]) {
        let recentMetrics = getRecentMetrics(minutes: 5)
        guard !recentMetrics.isEmpty else {
            return ("UNKNOWN", 0, ["No recent metrics available"])
        }

        let avgThompson = recentMetrics.map { $0.thompsonResponseTime }.reduce(0, +) / Double(recentMetrics.count)
        let avgMemory = recentMetrics.map { $0.memoryUsage }.reduce(0, +) / Double(recentMetrics.count)
        let avgAPI = recentMetrics.map { $0.apiResponseTime }.reduce(0, +) / Double(recentMetrics.count)
        let avgErrorRate = recentMetrics.map { $0.errorRate }.reduce(0, +) / Double(recentMetrics.count)

        var score = 100
        var issues: [String] = []

        if avgThompson > config.thompsonThresholdMs {
            score -= 30
            issues.append("Thompson performance violation")
        } else if avgThompson > config.thompsonThresholdMs * 0.8 {
            score -= 10
            issues.append("Thompson performance approaching limit")
        }

        if avgMemory > config.memoryCriticalMB {
            score -= 25
            issues.append("Memory usage critical")
        } else if avgMemory > config.memoryWarningMB {
            score -= 10
            issues.append("Memory usage elevated")
        }

        if avgAPI > config.apiTimeoutMs {
            score -= 20
            issues.append("API performance degraded")
        }

        if avgErrorRate > 0.05 {
            score -= 15
            issues.append("Error rate elevated")
        }

        let overall = score >= 90 ? "EXCELLENT" : score >= 75 ? "GOOD" : score >= 60 ? "ACCEPTABLE" : "POOR"
        return (overall, max(score, 0), issues)
    }

    func getRecentMetrics(minutes: Int) -> [PerformanceMetrics] {
        let cutoff = Date().addingTimeInterval(-Double(minutes) * 60)
        return metricsHistory.filter { $0.timestamp > cutoff }
    }

    func getActiveAlerts() -> [Alert] {
        let cutoff = Date().addingTimeInterval(-3600) // Last hour
        return alerts.filter { $0.timestamp > cutoff }
    }

    func generateReport() -> String {
        let health = getHealthStatus()
        let recentAlerts = getActiveAlerts()
        let recentMetrics = getRecentMetrics(minutes: 15)

        var report = """
        ðŸ“Š PERFORMANCE MONITORING REPORT
        ===============================

        ðŸ¥ System Health: \(health.overall) (Score: \(health.score)/100)
        ðŸ“ˆ Metrics Collected: \(metricsHistory.count) total, \(recentMetrics.count) recent
        ðŸš¨ Active Alerts: \(recentAlerts.count)

        """

        if !health.issues.isEmpty {
            report += "âš ï¸ Current Issues:\n"
            for issue in health.issues {
                report += "   â€¢ \(issue)\n"
            }
            report += "\n"
        }

        if !recentAlerts.isEmpty {
            report += "ðŸš¨ Recent Alerts:\n"
            for alert in recentAlerts.suffix(5) {
                let icon = getAlertIcon(alert.level)
                report += "   \(icon) \(alert.message)\n"
            }
            report += "\n"
        }

        if !recentMetrics.isEmpty {
            let avgThompson = recentMetrics.map { $0.thompsonResponseTime }.reduce(0, +) / Double(recentMetrics.count)
            let avgMemory = recentMetrics.map { $0.memoryUsage }.reduce(0, +) / Double(recentMetrics.count)
            let avgAPI = recentMetrics.map { $0.apiResponseTime }.reduce(0, +) / Double(recentMetrics.count)

            report += """
            ðŸ“Š Recent Performance (15 min avg):
               âš¡ Thompson: \(String(format: "%.3f", avgThompson))ms
               ðŸ’¾ Memory: \(String(format: "%.1f", avgMemory))MB
               ðŸŒ API: \(String(format: "%.0f", avgAPI))ms

            """
        }

        return report
    }

    private func getAlertIcon(_ level: AlertLevel) -> String {
        switch level {
        case .info: return "â„¹ï¸"
        case .warning: return "âš ï¸"
        case .critical: return "ðŸ”´"
        case .emergency: return "ðŸš¨"
        }
    }
}

print("ðŸŽ¯ MONITORING SYSTEM CONFIGURATION:")
print("   âš¡ Thompson Threshold: 10.0ms")
print("   ðŸ’¾ Memory Baseline: 200.0MB")
print("   ðŸŸ¡ Memory Warning: 170.0MB")
print("   ðŸ”´ Memory Critical: 190.0MB")
print("   ðŸŒ API Timeout: 5000.0ms")
print("   ðŸ“Š Metrics Retention: 24 hours")
print("")

// Initialize monitoring system
let monitor = PerformanceMonitoringSystem()

print("ðŸ“ˆ PHASE 1: BASELINE MONITORING VALIDATION")
print("==========================================")

// Simulate normal operation metrics
let normalMetrics = [
    PerformanceMetrics(timestamp: Date(), thompsonResponseTime: 0.028, memoryUsage: 150.0, apiResponseTime: 2500, cacheHitRate: 0.94, errorRate: 0.01, throughput: 400),
    PerformanceMetrics(timestamp: Date().addingTimeInterval(60), thompsonResponseTime: 0.032, memoryUsage: 152.0, apiResponseTime: 2300, cacheHitRate: 0.93, errorRate: 0.015, throughput: 420),
    PerformanceMetrics(timestamp: Date().addingTimeInterval(120), thompsonResponseTime: 0.029, memoryUsage: 149.0, apiResponseTime: 2800, cacheHitRate: 0.95, errorRate: 0.008, throughput: 380)
]

for metrics in normalMetrics {
    monitor.recordMetrics(metrics)
}

let baselineHealth = monitor.getHealthStatus()
print("   ðŸ“Š Baseline Health: \(baselineHealth.overall) (\(baselineHealth.score)/100)")
print("   ðŸš¨ Baseline Alerts: \(monitor.getActiveAlerts().count)")
print("   âœ… Status: Normal operation monitoring functional")
print("")

print("âš ï¸  PHASE 2: WARNING CONDITION SIMULATION")
print("=========================================")

// Simulate warning conditions
let warningMetrics = [
    PerformanceMetrics(timestamp: Date().addingTimeInterval(180), thompsonResponseTime: 8.5, memoryUsage: 175.0, apiResponseTime: 3200, cacheHitRate: 0.87, errorRate: 0.03, throughput: 320),
    PerformanceMetrics(timestamp: Date().addingTimeInterval(240), thompsonResponseTime: 9.2, memoryUsage: 182.0, apiResponseTime: 3800, cacheHitRate: 0.85, errorRate: 0.04, throughput: 300)
]

for metrics in warningMetrics {
    monitor.recordMetrics(metrics)
}

let warningHealth = monitor.getHealthStatus()
let warningAlerts = monitor.getActiveAlerts()

print("   ðŸ“Š Warning Health: \(warningHealth.overall) (\(warningHealth.score)/100)")
print("   ðŸš¨ Generated Alerts: \(warningAlerts.count)")

for alert in warningAlerts.suffix(3) {
    let icon = alert.level == .warning ? "âš ï¸" : "ðŸ”´"
    print("      \(icon) \(alert.metric): \(String(format: "%.1f", alert.value)) (threshold: \(String(format: "%.1f", alert.threshold)))")
}
print("")

print("ðŸš¨ PHASE 3: CRITICAL CONDITION SIMULATION")
print("=========================================")

// Simulate critical conditions (but not emergency)
let criticalMetrics = [
    PerformanceMetrics(timestamp: Date().addingTimeInterval(300), thompsonResponseTime: 9.8, memoryUsage: 195.0, apiResponseTime: 4800, cacheHitRate: 0.75, errorRate: 0.08, throughput: 200)
]

for metrics in criticalMetrics {
    monitor.recordMetrics(metrics)
}

let criticalHealth = monitor.getHealthStatus()
let criticalAlerts = monitor.getActiveAlerts().filter { $0.level == .critical || $0.level == .emergency }

print("   ðŸ“Š Critical Health: \(criticalHealth.overall) (\(criticalHealth.score)/100)")
print("   ðŸ”´ Critical Alerts: \(criticalAlerts.count)")

for alert in criticalAlerts {
    print("      ðŸ”´ \(alert.message)")
}
print("")

print("âœ… PHASE 4: RECOVERY MONITORING")
print("===============================")

// Simulate system recovery
let recoveryMetrics = [
    PerformanceMetrics(timestamp: Date().addingTimeInterval(360), thompsonResponseTime: 0.035, memoryUsage: 155.0, apiResponseTime: 2200, cacheHitRate: 0.92, errorRate: 0.012, throughput: 390)
]

for metrics in recoveryMetrics {
    monitor.recordMetrics(metrics)
}

let recoveryHealth = monitor.getHealthStatus()
print("   ðŸ“Š Recovery Health: \(recoveryHealth.overall) (\(recoveryHealth.score)/100)")
print("   ðŸ”„ Recovery Status: \(recoveryHealth.issues.isEmpty ? "âœ… FULL RECOVERY" : "âš ï¸ PARTIAL RECOVERY")")
print("")

print("ðŸ“Š PHASE 5: COMPREHENSIVE MONITORING REPORT")
print("===========================================")

let finalReport = monitor.generateReport()
print(finalReport)

print("ðŸ† MONITORING SYSTEM VALIDATION RESULTS")
print("=======================================")

let validationScore = assessMonitoringSystem(monitor)

print("   ðŸ“Š Monitoring Capabilities:")
print("      ðŸŽ¯ Threshold Detection: \(validationScore.thresholdDetection ? "âœ…" : "âŒ")")
print("      ðŸš¨ Alert Generation: \(validationScore.alertGeneration ? "âœ…" : "âŒ")")
print("      ðŸ“ˆ Health Scoring: \(validationScore.healthScoring ? "âœ…" : "âŒ")")
print("      ðŸ“Š Metrics Collection: \(validationScore.metricsCollection ? "âœ…" : "âŒ")")
print("      ðŸ”„ Recovery Tracking: \(validationScore.recoveryTracking ? "âœ…" : "âŒ")")
print("")

print("   ðŸŽ¯ Overall Monitoring Score: \(validationScore.score)/100")
print("   ðŸ“ˆ Grade: \(validationScore.grade)")
print("   ðŸš€ Status: \(validationScore.status)")
print("")

if validationScore.score >= 90 {
    print("ðŸŽ‰ EXCELLENT! Performance monitoring system is production-ready!")
    print("Real-time monitoring provides comprehensive coverage of all critical metrics.")
} else if validationScore.score >= 75 {
    print("âœ… GOOD! Monitoring system is functional with minor improvements needed.")
} else {
    print("âš ï¸  IMPROVEMENT NEEDED! Monitoring system requires optimization.")
}

print("")
print("=================================================")
print("Performance Monitoring Validation Complete âœ…")
print("=================================================")

// Validation Assessment Function
struct MonitoringValidationScore {
    let thresholdDetection: Bool
    let alertGeneration: Bool
    let healthScoring: Bool
    let metricsCollection: Bool
    let recoveryTracking: Bool
    let score: Int
    let grade: String
    let status: String
}

func assessMonitoringSystem(_ monitor: PerformanceMonitoringSystem) -> MonitoringValidationScore {
    let alerts = monitor.getActiveAlerts()
    let health = monitor.getHealthStatus()
    let metrics = monitor.getRecentMetrics(minutes: 30)

    // Check capabilities
    let thresholdDetection = alerts.contains { $0.level == .warning || $0.level == .critical }
    let alertGeneration = !alerts.isEmpty
    let healthScoring = health.score >= 0 && health.score <= 100
    let metricsCollection = !metrics.isEmpty
    let recoveryTracking = health.overall != "UNKNOWN"

    // Calculate score
    var score = 0
    if thresholdDetection { score += 25 }
    if alertGeneration { score += 20 }
    if healthScoring { score += 20 }
    if metricsCollection { score += 20 }
    if recoveryTracking { score += 15 }

    let grade = score >= 90 ? "A EXCELLENT" : score >= 80 ? "B VERY GOOD" : score >= 70 ? "C GOOD" : "D NEEDS IMPROVEMENT"
    let status = score >= 80 ? "PRODUCTION READY" : score >= 60 ? "NEEDS MINOR IMPROVEMENTS" : "REQUIRES OPTIMIZATION"

    return MonitoringValidationScore(
        thresholdDetection: thresholdDetection,
        alertGeneration: alertGeneration,
        healthScoring: healthScoring,
        metricsCollection: metricsCollection,
        recoveryTracking: recoveryTracking,
        score: score,
        grade: grade,
        status: status
    )
}